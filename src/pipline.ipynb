{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "from collections import deque\n",
    "from model import TransferBiLSTM  # your updated model\n",
    "\n",
    "# Load trained multiclass model\n",
    "model = TransferBiLSTM(input_size=132, hidden_size=64, num_layers=2, num_classes=5)\n",
    "model.load_state_dict(torch.load('pain_model_multiclass.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Mediapipe setup\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic()\n",
    "seq_len = 8\n",
    "landmark_buffer = deque(maxlen=seq_len)\n",
    "\n",
    "def extract_landmarks(results):\n",
    "    pose = results.pose_landmarks\n",
    "    face = results.face_landmarks\n",
    "    lh = results.left_hand_landmarks\n",
    "    rh = results.right_hand_landmarks\n",
    "\n",
    "    def flatten_landmarks(landmarks, n=33):\n",
    "        if landmarks:\n",
    "            return np.array([[lmk.x, lmk.y, lmk.z, lmk.visibility] for lmk in landmarks.landmark]).flatten()\n",
    "        else:\n",
    "            return np.zeros(n * 4)\n",
    "\n",
    "    return np.concatenate([\n",
    "        flatten_landmarks(pose, 33),\n",
    "        flatten_landmarks(face, 468),\n",
    "        flatten_landmarks(lh, 21),\n",
    "        flatten_landmarks(rh, 21)\n",
    "    ])[:132]  # adjust if your model uses full landmark size\n",
    "\n",
    "# Webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image)\n",
    "    \n",
    "    landmarks = extract_landmarks(results)\n",
    "    landmark_buffer.append(landmarks)\n",
    "\n",
    "    if len(landmark_buffer) == seq_len:\n",
    "        input_seq = torch.tensor([list(landmark_buffer)], dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_seq)\n",
    "            probs = torch.softmax(logits, dim=1).numpy()[0]\n",
    "            predicted_class = int(np.argmax(probs))\n",
    "            confidence = probs[predicted_class]\n",
    "\n",
    "        # Display result\n",
    "        text = f'Pain Class: {predicted_class} ({confidence:.2f})'\n",
    "        cv2.putText(frame, text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                    (0, 0, 255) if predicted_class > 0 else (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Real-Time Pain Detection\", frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d0f42",
   "metadata": {},
   "source": [
    "# Full pipeline initial \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729699cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load both models\n",
    "image_model = torch.load(\"/Users/suryanshpatel/Projects/Directed Readings/Technical/src/Models/best_model.pth\")\n",
    "lstm_model = torch.load(\"/Users/suryanshpatel/Projects/Directed Readings/Technical/src/Models/gru_classifier.pth\")\n",
    "\n",
    "image_model.eval()\n",
    "lstm_model.eval()\n",
    "\n",
    "# Setup Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# --- IMAGE PROCESSING (2-ch grayscale) ---\n",
    "def get_2ch_image_from_frame(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (48, 48))\n",
    "\n",
    "    edges = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    edges = np.uint8(np.absolute(edges))\n",
    "\n",
    "    stacked = np.stack([gray, edges], axis=0)\n",
    "    return torch.tensor(stacked, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "\n",
    "# --- LANDMARK PROCESSING ---\n",
    "def extract_landmark_vector(results):\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = [[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]\n",
    "        flat = np.array(landmarks).flatten()\n",
    "        return torch.tensor(flat, dtype=torch.float32).unsqueeze(0)\n",
    "    return None\n",
    "\n",
    "# Real-time prediction\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "sequence = []\n",
    "sequence_length = 30  # how many frames per sequence\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image)\n",
    "\n",
    "    # --- Landmark Model Inference ---\n",
    "    lm_vector = extract_landmark_vector(results)\n",
    "    if lm_vector is not None:\n",
    "        sequence.append(lm_vector)\n",
    "        if len(sequence) > sequence_length:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            landmark_input = torch.stack(sequence).unsqueeze(0)  # shape: (1, 30, 99*3)\n",
    "            with torch.no_grad():\n",
    "                pred_lm = lstm_model(landmark_input)\n",
    "                prob_lm = torch.softmax(pred_lm, dim=1)\n",
    "    else:\n",
    "        prob_lm = torch.zeros(1, 5)\n",
    "\n",
    "    # --- Image Model Inference ---\n",
    "    image_tensor = get_2ch_image_from_frame(frame)\n",
    "    with torch.no_grad():\n",
    "        pred_img = image_model(image_tensor)\n",
    "        prob_img = torch.softmax(pred_img, dim=1)\n",
    "\n",
    "    # --- Combine Predictions ---\n",
    "    combined_prob = (prob_img + prob_lm) / 2\n",
    "    final_class = torch.argmax(combined_prob, dim=1).item()\n",
    "\n",
    "    # --- Display ---\n",
    "    cv2.putText(frame, f\"Pain Level: {final_class}\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Pain Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
